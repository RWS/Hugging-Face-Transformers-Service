fastapi==0.112.1
transformers==4.44.0
torch==2.4.1
uvicorn==0.30.6
python-multipart==0.0.17
websockets==14.1
huggingface_hub==0.24.6
sentencepiece==0.2.0
numpy==1.26.4
sacremoses==0.1.1
sse-starlette==2.1.3
python-dotenv==1.0.1
pyinstaller==6.11.0
accelerate==1.1.1
# --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
# llama-cpp-python